"""
Runtime orchestrator for PIE backend.

This module provides the main Runtime class that orchestrates inference
by delegating to specialized components:
- config.py: RuntimeConfig
- batching.py: BatchBuilder, BatchState
- loader.py: ModelLoader
"""

from __future__ import annotations

import base64
import random
from pathlib import Path
import time
from typing import TYPE_CHECKING

import numpy as np
from . import utils
import torch
import torch.distributed as dist

from .config import RuntimeConfig
from .batching import BatchBuilder, Batch
from .loader import ModelLoader
from .adapter import AdapterSubpass, CmaesAdapter
from .model import llama3, qwen2, qwen3, common, gpt_oss
from . import message

# Re-export RuntimeConfig for backward compatibility
__all__ = ["Runtime", "RuntimeConfig"]


class Runtime:
    """
    Main runtime orchestrator for PIE inference.
    
    This class has been refactored to focus on protocol handling,
    delegating loading to ModelLoader and batch management to BatchBuilder.
    """

    config: RuntimeConfig
    
    # Model components (renamed from forward_pass to engine to avoid collision)
    engine: llama3.ForwardPass
    model_config: llama3.ModelConfig
    kv_cache_at_layer: list[torch.Tensor]
    
    # Adapter state
    adapter_at_layer: list[tuple[torch.Tensor, torch.Tensor]]
    adapters: dict
    
    # Batch management
    batch_builder: BatchBuilder
    batch: Batch | None

    def __init__(self, config: RuntimeConfig):
        """
        Initialize the runtime.
        
        Args:
            config: Runtime configuration
        """
        self.config = config
        self.adapters = {}
        self.batch = None

        # Initialize seeds
        print(f"Initializing with random seed: {config.random_seed}")
        random.seed(config.random_seed)
        np.random.seed(config.random_seed)
        torch.manual_seed(config.random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(config.random_seed)

        # Load model weights using ModelLoader
        loader = ModelLoader(config)
        print("Loading model weights")
        weights, normalized_arch, self.info = loader.load()

        print("Loaded model weights")

        # Store architecture type
        self.type = self.info["architecture"]["type"]

        # Create model-specific components based on architecture
        match self.type:
            case "llama3" | "l4ma":
                # Create model config
                self.model_config = llama3.ModelConfig.from_dict(normalized_arch)
                
                # Create forward pass with weights
                self.engine = llama3.ForwardPass(
                    self.model_config,
                    config,
                    weights,
                )
                # Create adapter cache
                self.adapter_at_layer = llama3.create_adapter_cache(
                    self.model_config, config
                )
                # Evaluate and store max_num_kv_pages in config
                config.max_num_kv_pages = self.model_config.eval_max_num_kv_pages(config)
                # Create KV cache
                self.kv_cache_at_layer = llama3.create_kv_cache(
                    self.model_config, config
                )
                
                

            case "qwen2":
                # Create model config
                self.model_config = qwen2.ModelConfig.from_dict(normalized_arch)
                
                # Create forward pass with weights
                self.engine = qwen2.ForwardPass(
                    self.model_config,
                    config,
                    weights,
                )
                # Create adapter cache
                self.adapter_at_layer = qwen2.create_adapter_cache(
                    self.model_config, config
                )
                # Evaluate and store max_num_kv_pages in config
                config.max_num_kv_pages = self.model_config.eval_max_num_kv_pages(config)
                # Create KV cache
                self.kv_cache_at_layer = qwen2.create_kv_cache(
                    self.model_config, config
                )
                

                
            case "qwen3":
                # Create model config
                self.model_config = qwen3.ModelConfig.from_dict(normalized_arch)
                
                # Create forward pass with weights
                # Create forward pass with weights
                self.engine = qwen3.ForwardPass(
                    self.model_config,
                    config,
                    weights,
                )

                # Create adapter cache
                self.adapter_at_layer = qwen3.create_adapter_cache(
                    self.model_config, config
                )
                # Evaluate and store max_num_kv_pages in config
                config.max_num_kv_pages = self.model_config.eval_max_num_kv_pages(config)
                # Create KV cache
                self.kv_cache_at_layer = qwen3.create_kv_cache(
                    self.model_config, config
                )
                

                
            case "gptoss":
                # Create model config
                self.model_config = gpt_oss.ModelConfig.from_dict(normalized_arch)
                
                # Create forward pass with weights
                self.engine = gpt_oss.ForwardPass(
                    self.model_config,
                    config,
                    weights,
                )
                # Create adapter cache
                self.adapter_at_layer = gpt_oss.create_adapter_cache(
                    self.model_config, config
                )
                # Evaluate and store max_num_kv_pages in config
                config.max_num_kv_pages = self.model_config.eval_max_num_kv_pages(config)
                # Create KV cache
                self.kv_cache_at_layer = gpt_oss.create_kv_cache(
                    self.model_config, config
                )

            case _:
                raise ValueError(f"Unsupported architecture type: {self.type}")

        # Initialize batch builder
        self.batch_builder = BatchBuilder(
            kv_page_size=config.kv_page_size,
            max_dist_size=config.max_dist_size,
            adapters=self.adapters,
        )
    



    # ========================================================================
    # Metadata Accessors
    # ========================================================================

    def get_metadata(self) -> dict:
        """Get model metadata."""
        return {
            "name": self.info.get("name", self.config.model),
            "description": self.info.get("description", ""),
            "version": self.info.get("version", "1.0.0"),
        }

    def get_chat_template(self) -> dict:
        """Get chat template configuration."""
        template = self.info.get("template", {})
        return {
            "template_type": template.get("type", "none"),
            "template_content": template.get("content", ""),
            "stop_tokens": template.get("stop_tokens", []),
        }

    def get_tokenizer(self) -> dict:
        """Get tokenizer configuration with merge table."""
        tokenizer_info = self.info.get("tokenizer", {})
        model_dir = Path(self.config.cache_dir) / "models" / self.config.model
        #print("model_dir", model_dir)
        # Get vocab file path
        vocab_filename = tokenizer_info.get("vocab") or tokenizer_info.get(
            "vocabulary_file"
        )
        if not vocab_filename:
            # Return minimal tokenizer info if no vocab file
            return {
                "type": tokenizer_info.get("type", "bpe"),
                "num_vocab": tokenizer_info.get(
                    "vocab_size", self.engine.weights.get("embed_token").shape[0]
                ),
                "merge_table": {},
                "split_regex": tokenizer_info.get("split_regex", ""),
                "special_tokens": tokenizer_info.get("special_tokens", {}),
                "escape_non_printable": tokenizer_info.get(
                    "escape_non_printable", False
                ),
            }

        vocab_file_path = model_dir / vocab_filename
        merge_rules: dict[int, bytes] = {}

        if vocab_file_path.exists():
            with open(vocab_file_path, "r", encoding="utf-8") as f:
                for line_number, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split()
                    if len(parts) != 2:
                        continue
                    b64_token, rank_str = parts
                    try:
                        decoded_token = base64.b64decode(b64_token)
                        rank = int(rank_str)
                        merge_rules[rank] = decoded_token
                    except (ValueError, TypeError):
                        continue

        return {
            "type": tokenizer_info.get("type", "bpe"),
            "num_vocab": tokenizer_info.get(
                "vocab_size", self.engine.weights.get("embed_token").shape[0]
            ),
            "merge_table": merge_rules,
            "split_regex": tokenizer_info.get("split_regex", ""),
            "special_tokens": tokenizer_info.get("special_tokens", {}),
            "escape_non_printable": tokenizer_info.get("escape_non_printable", False),
        }

    # ========================================================================
    # Service Protocol Implementation
    # ========================================================================

    def handshake(
        self, reqs: list[message.HandshakeRequest]
    ) -> list[message.HandshakeResponse]:
        """Handle handshake requests returning model and tokenizer info."""
        metadata = self.get_metadata()
        template = self.get_chat_template()
        tokenizer = self.get_tokenizer()
        
        responses = []
        for _ in reqs:
            resp = message.HandshakeResponse(
                version=metadata.get("version", "1.0.0"),
                model_name=metadata["name"],
                model_traits=[],  # TODO: populate traits
                model_description=metadata.get("description", ""),
                prompt_template=template["template_content"],
                prompt_template_type=template["template_type"],
                prompt_stop_tokens=template["stop_tokens"],
                kv_page_size=self.config.kv_page_size,
                max_batch_tokens=self.config.max_batch_tokens or 10240,
                resources={
                    0: self.config.max_num_kv_pages or 0,
                    1: self.config.max_num_embeds,
                    2: self.config.max_num_adapters,
                },
                tokenizer_num_vocab=tokenizer["num_vocab"],
                tokenizer_merge_table=tokenizer["merge_table"],
                tokenizer_special_tokens=tokenizer["special_tokens"],
                tokenizer_split_regex=tokenizer["split_regex"],
                tokenizer_escape_non_printable=tokenizer["escape_non_printable"],
            )
            responses.append(resp)
        return responses



    def query(
        self, reqs: list[message.QueryRequest]
    ) -> list[message.QueryResponse]:
        """Handle query requests."""
        responses = []
        for req in reqs:
            value = "unknown query"
            match req.query:
                case "ping":
                    value = "pong"
            responses.append(message.QueryResponse(value=value))
        return responses

    def forward_pass_handler(
        self, reqs: list[message.ForwardPassRequest]
    ) -> list[message.ForwardPassResponse]:
        """
        Handle batched forward pass inference requests.
        
        Note: This method was renamed from forward_pass to avoid collision
        with the forward_pass property. For backward compatibility, the
        server.py handler should call this method.
        """
        # Accumulate requests into the batch
        for req in reqs:
            self.batch_builder.add_request(req)

        # Execute the batch and return responses
        return self._execute_batch()

    def embed_image(self, reqs: list[message.EmbedImageRequest]) -> None:
        """Handle image embedding requests."""
        # TODO: implement image embedding
        pass

    def initialize_adapter(
        self, reqs: list[message.InitializeAdapterRequest]
    ) -> None:
        """Initialize adapter functionality."""
        for req in reqs:
            adapter_ptr = req.adapter_ptr
            # Prepare args for initialization
            args = {
                "adapter_ptr": adapter_ptr,
                "rank": req.rank,
                "alpha": req.alpha,
                "population_size": req.population_size,
                "mu_fraction": req.mu_fraction,
                "initial_sigma": req.initial_sigma,
            }

            if self.config.world_size > 1:
                # Broadcast INIT_ADAPTER command
                msg = {
                    "type": "INIT_ADAPTER",
                    "kwargs": args
                }
                utils.broadcast_struct(msg, src=0, device=self.config.device)

            self._initialize_adapter(**args)

    def update_adapter(self, reqs: list[message.UpdateAdapterRequest]) -> None:
        """Update adapter parameters."""
        for req in reqs:
            args = {
                "adapter_ptr": req.adapter_ptr,
                "scores": req.scores,
                "seeds": req.seeds,
                "max_sigma": req.max_sigma,
            }

            if self.config.world_size > 1:
                # Broadcast UPDATE_ADAPTER command
                msg = {
                    "type": "UPDATE_ADAPTER",
                    "kwargs": args
                }
                utils.broadcast_struct(msg, src=0, device=self.config.device)

            self._update_adapter(**args)

    def upload_adapter(self, reqs: list[message.UploadAdapterRequest]) -> None:
        """Upload adapter weights."""
        for req in reqs:
            if req.adapter_ptr in self.adapters:
                adapter = self.adapters[req.adapter_ptr]
                if isinstance(adapter, CmaesAdapter):
                    adapter.upload(req.name, req.adapter_data)

    def download_adapter(
        self, reqs: list[message.DownloadAdapterRequest]
    ) -> list[message.DownloadAdapterResponse]:
        """Download adapter weights."""
        resps = []
        for req in reqs:
            if req.adapter_ptr in self.adapters:
                adapter = self.adapters[req.adapter_ptr]
                if isinstance(adapter, CmaesAdapter):
                    data = adapter.download(req.name)
                    resp = message.DownloadAdapterResponse(adapter_data=data)
                    resps.append(resp)
        return resps

    # ========================================================================
    # Internal Adapter Methods
    # ========================================================================

    @torch.inference_mode()
    def _initialize_adapter(
        self,
        adapter_ptr: int,
        rank: int,
        alpha: float,
        population_size: int,
        mu_fraction: float,
        initial_sigma: float,
    ):
        cfg = self.model_config
        
        # Check if adapter limits are exceeded
        if adapter_ptr >= self.config.max_num_adapters:
             raise ValueError(f"Adapter pointer {adapter_ptr} exceeds max_num_adapters {self.config.max_num_adapters}")
        # print parameters
        #print(f"Initializing adapter {adapter_ptr} with rank {rank}, alpha {alpha}, population size {population_size}, mu fraction {mu_fraction}, initial sigma {initial_sigma}")
        # Calculate local shard sizes for distributed adapters
        world_size = self.config.world_size
        gpu_rank = self.config.rank
        
        local_num_q_heads = cfg.num_q_heads // world_size
        local_num_kv_heads = cfg.num_kv_heads // world_size
        
        # Local output features (sharded up-projection)
        local_out_features = [
            cfg.dim_head * local_num_q_heads,
            cfg.dim_head * local_num_kv_heads,
            cfg.dim_head * local_num_kv_heads
        ]

        # Initialize adapter
        self.adapters[adapter_ptr] = CmaesAdapter(
            adapter_id=adapter_ptr,
            adapter_at_layer=self.adapter_at_layer,
            rank=rank,
            alpha=alpha,
            in_features=cfg.dim_hidden,
            out_features=local_out_features,
            num_layers=cfg.num_layers,
            population_size=population_size,
            mu_fraction=mu_fraction,
            initial_sigma=initial_sigma,
            min_sigma=1e-7,
            min_var=1e-8,
            max_var=1e4,
            device=self.config.device,
            dtype=self.config.activation_dtype,
            gpu_rank=gpu_rank,
            world_size=world_size,
        )

    @torch.inference_mode()
    def _update_adapter(
        self,
        adapter_ptr: int,
        scores: list[float],
        seeds: list[int],
        max_sigma: float,
    ):
        if adapter_ptr in self.adapters:
            adapter = self.adapters[adapter_ptr]
            if isinstance(adapter, CmaesAdapter):
                adapter.update(scores, seeds, max_sigma)

    # ========================================================================
    # Batch Execution
    # ========================================================================

    @torch.inference_mode()
    def worker_loop(self):
        """
        Worker loop for ranks > 0.
        Waits for control messages from rank 0 and executes commands.
        """
        import torch.distributed as dist
        import signal
        
        # Re-enable SIGTERM handling so this worker can be terminated
        # by the parent process during shutdown
        shutdown_requested = False
        
        def sigterm_handler(signum, frame):
            nonlocal shutdown_requested
            shutdown_requested = True
        
        signal.signal(signal.SIGTERM, sigterm_handler)
        signal.signal(signal.SIGINT, sigterm_handler)
        
        device = self.config.device
        
        while not shutdown_requested:
            # Receive control message
            try:
                msg = utils.broadcast_struct(None, src=0, device=device)
            except Exception:
                break
                
            if shutdown_requested:
                break
                
            if msg == "STOP":
                break
                
            if isinstance(msg, dict):
                msg_type = msg.get("type")
                
                if msg_type == "STEP":
                    # Execute inference step
                    inputs = msg["inputs"]
                    sampling_metadata = msg["sampling_metadata"]
                    try:
                        self._run_step(inputs, sampling_metadata)
                    except Exception as e:
                        # Log error but continue - don't let one error hang the whole system
                        print(f"Worker {self.config.rank} _run_step error: {e}")
                        # Sync CUDA to clear any pending operations
                        torch.cuda.synchronize()
                    
                elif msg_type == "INIT_ADAPTER":
                    # Initialize adapter
                    kwargs = msg["kwargs"]
                    self._initialize_adapter(**kwargs)

                elif msg_type == "UPDATE_ADAPTER":
                    # Update adapter
                    kwargs = msg["kwargs"]
                    self._update_adapter(**kwargs)
                    
            # Other message types can be added here

    def _run_step(self, inputs: dict, sampling_metadata: dict) -> list:
        """
        Execute a single inference step (Embed -> Transform -> Sample).
        
        Returns:
            Sampling results (only valid on Rank 0 usually, but we return whatever comes out)
        """
        if self.config.world_size > 1:
            torch.distributed.barrier()



        # 2. Embed inputs
        input_embeds = self.engine.embed_inputs(inputs)
        
        # 3. Use raw indices/seeds to create local AdapterSubpass (to avoid device mismatch)
        adapter_subpass = None
        if inputs.get("adapter_indices"):
            adapter_subpass = AdapterSubpass(
                adapter_at_layer=self.adapter_at_layer,
                adapter_indices=inputs["adapter_indices"],
                adapter_extras=self.adapters,
                rand_seeds=inputs["adapter_seeds"],
                qo_indptr=inputs["qo_indptr"],
            )

        # 4. Run transformer forward pass
        hidden_states = self.engine.transform(
            input_embeds=input_embeds,
            position_ids=inputs["position_ids"],
            qo_indptr=inputs["qo_indptr"],
            kv_cache_at_layer=self.kv_cache_at_layer,
            kv_page_indices=inputs["kv_page_indices"],
            kv_page_indptr=inputs["kv_page_indptr"],
            kv_last_page_lens=inputs["kv_last_page_lens"],
            custom_mask=inputs["custom_mask"],
            single_token_inference_mode=inputs["single_token_inference_mode"],
            adapter_subpass=adapter_subpass,
        )
        
        # 4. Sampling Pass
        # Pass hidden_states (replicated or sharded? Transform returns replicated in current llama3.py)
        sampling_results = self.engine.sample(hidden_states, sampling_metadata)
        
        return sampling_results

    @torch.inference_mode()
    def _execute_batch(self) -> list[message.ForwardPassResponse]:
        """
        Execute the accumulated batch and return responses.
        """
        batch = self.batch_builder.build()
        device = self.config.device
        sampling_metadata = batch.get_sampling_metadata(device, self.config.activation_dtype)


        
        inputs = batch.get_model_inputs(device)
        
        # Broadcast if needed
        if self.config.world_size > 1:
            # Broadcast Step command
            msg = {
                "type": "STEP",
                "inputs": inputs,
                "sampling_metadata": sampling_metadata
            }

            utils.broadcast_struct(msg, src=0, device=device)

        # Execute step
        sampling_results = self._run_step(inputs, sampling_metadata)

        # Package responses
        responses = batch.create_responses(sampling_results)

        return responses

    def shutdown(self):
        """
        Cleanup runtime resources.
        
        For Rank 0 in multi-GPU setup, this broadcasts the 'STOP' signal to workers.
        """
        if self.config.world_size > 1 and self.config.rank == 0:
            print("Broadcasting STOP signal to workers...")
            utils.broadcast_struct("STOP", src=0, device=self.config.device)

