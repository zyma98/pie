[package]
name = "windowed-attention"
version = "0.1.0"
description = "Sliding window attention for KV cache management"
authors = ["Pie Team"]

[runtime]
core = "^0.2.0"
mcp = "^0.2.0"

[parameters]
prompt = {type = "string", optional = true, description = "The prompt to send to the model (default: 'Explain LLM decoding process in ELI5.')"}
max_tokens = {type = "int", optional = true, description = "Maximum number of new tokens to generate (default: 512)"}
window_size = {type = "int", optional = true, description = "Sliding window size for KV cache (default: 32)"}
