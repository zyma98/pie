"""
Metal attention operation implementation.

This module handles compilation and execution of Metal attention kernels
for PyTorch MPS backend.
"""

import torch
from typing import Optional
from .mps_shader_compiler import BaseShaderCompiler
from .mps_config import (
    MPS_COMPILE_AVAILABLE,
    DEBUG_ENABLED,
    DEBUG_ATOL,
    DEBUG_RTOL,
    DEBUG_VERBOSITY,
    VERBOSITY_DETAILED
)


class AttentionCompiler(BaseShaderCompiler):
    """Compiles and runs Metal attention kernels."""

    def __init__(self):
        super().__init__()
        self._compile_attention_kernels()

    def _compile_attention_kernels(self):
        """Compile the actual optimized Metal attention kernels."""
        if not MPS_COMPILE_AVAILABLE:
            return

        # Read the actual optimized kernel sources
        common_source = self._read_metal_file("metal_attention_common.metal")
        simdgroup_source = self._read_metal_file("metal_attention_simdgroup_opt.metal")

        if not common_source or not simdgroup_source:
            print("⚠️  Could not find optimized Metal kernel sources, using fallback")
            self._compile_simple_kernels()
            return

        # Process the common header to resolve includes
        processed_common = self._process_common_header(common_source)

        # Process the simdgroup source to resolve includes
        processed_simdgroup = self._resolve_includes(simdgroup_source, processed_common)

        # Transform Params struct to params_raw for torch.mps.compile_shader compatibility
        processed_simdgroup = processed_simdgroup.replace(
            'constant Params& params [[buffer(8)]]',
            'device const float* params_raw [[buffer(8)]]'
        )

        # Replace parameter access patterns
        param_replacements = [
            ('const int num_qo = params.num_qo;', 'const int num_qo = (int)params_raw[0];'),
            ('const int head_dim = params.head_dim;', 'const int head_dim = (int)params_raw[1];'),
            ('const int kv_head_dim = params.kv_head_dim;', 'const int kv_head_dim = (int)params_raw[2];'),
            ('const int head_size = params.head_size;', 'const int head_size = (int)params_raw[3];'),
            ('const int page_size = params.page_size;', 'const int page_size = (int)params_raw[4];'),
            ('const int num_query_heads = params.num_query_heads;', 'const int num_query_heads = (int)params_raw[5];'),
            ('const int num_kv_heads = params.num_kv_heads;', 'const int num_kv_heads = (int)params_raw[6];'),
            ('const float scale = params.scale;', 'const float scale = params_raw[7];'),
        ]

        for old, new in param_replacements:
            processed_simdgroup = processed_simdgroup.replace(old, new)

        # Compile the full optimized attention kernels
        full_source = f"""
#include <metal_stdlib>
using namespace metal;

{processed_common}

{processed_simdgroup}
"""

        try:
            # Compile the actual optimized shader library
            if self._compile_shader(full_source, 'attention'):
                print("✅ Compiled OPTIMIZED Metal attention kernels for MPS")
            else:
                print("   Falling back to simple implementation")
                self._compile_simple_kernels()

        except Exception as e:
            print(f"⚠️  Failed to compile optimized kernels: {e}")
            print("   Falling back to simple implementation")
            # Fall back to simpler implementation
            self._compile_simple_kernels()

    def _compile_simple_kernels(self):
        """Compile simplified kernels that work with torch.mps.compile_shader."""
        if not MPS_COMPILE_AVAILABLE:
            return

        # Very simple kernel that just works
        simple_source = """
#include <metal_stdlib>
using namespace metal;

struct SimpleParams {
    int num_tokens;
    int head_dim;
    float scale;
};

kernel void simple_attention_f32(
    device const float* query [[buffer(0)]],
    device const float* key [[buffer(1)]],
    device const float* value [[buffer(2)]],
    device float* output [[buffer(3)]],
    constant SimpleParams& params [[buffer(4)]],
    uint idx [[thread_position_in_grid]]
) {
    if (idx >= uint(params.num_tokens * params.head_dim)) return;

    uint token_idx = idx / uint(params.head_dim);
    uint dim_idx = idx % uint(params.head_dim);

    // Simple attention: just copy value (placeholder)
    output[idx] = value[idx] * params.scale;
}

kernel void simple_attention_f16(
    device const half* query [[buffer(0)]],
    device const half* key [[buffer(1)]],
    device const half* value [[buffer(2)]],
    device half* output [[buffer(3)]],
    constant SimpleParams& params [[buffer(4)]],
    uint idx [[thread_position_in_grid]]
) {
    if (idx >= uint(params.num_tokens * params.head_dim)) return;

    uint token_idx = idx / uint(params.head_dim);
    uint dim_idx = idx % uint(params.head_dim);

    // Simple attention: just copy value (placeholder)
    output[idx] = half(value[idx] * params.scale);
}
"""

        if self._compile_shader(simple_source, 'simple'):
            print("✅ Compiled simple attention kernels for MPS")

    def run_attention_mps(
        self,
        query: torch.Tensor,
        kv_cache: torch.Tensor,
        kv_page_indices: torch.Tensor,
        kv_page_indptr: torch.Tensor,
        kv_last_page_lens: torch.Tensor,
        qo_indptr: torch.Tensor,
        custom_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Run attention using compiled MPS kernels.

        All dtype conversions happen here before passing to Metal kernels.
        Metal kernels expect float16 (half) types.
        """

        if not self.can_use_mps_kernels():
            raise RuntimeError("MPS kernels not available")

        if 'attention' not in self.compiled_libraries:
            raise RuntimeError("Attention kernel not compiled")

        # DEBUG MODE: Collect input metadata before any modifications
        if DEBUG_ENABLED:
            from . import debug_utils
            from . import pytorch_reference

            input_metadata = [
                debug_utils.collect_tensor_metadata(query, "query"),
                debug_utils.collect_tensor_metadata(kv_cache, "kv_cache"),
                debug_utils.collect_tensor_metadata(kv_page_indices, "kv_page_indices"),
                debug_utils.collect_tensor_metadata(kv_page_indptr, "kv_page_indptr"),
                debug_utils.collect_tensor_metadata(kv_last_page_lens, "kv_last_page_lens"),
                debug_utils.collect_tensor_metadata(qo_indptr, "qo_indptr"),
            ]

        # Store original dtype for conversion back
        original_dtype = query.dtype

        # Dtype conversion logic:
        # - float32: Keep as-is, use f32 kernel
        # - float16: Keep as-is, use bf16 kernel (uses half type)
        # - bfloat16: Convert to float16, use bf16 kernel
        # The "bf16" kernel name is misleading - it actually uses Metal's half (fp16) type
        if query.dtype == torch.bfloat16:
            # BFloat16 is not natively supported in Metal, convert to float16
            query = query.to(torch.float16)
            kv_cache = kv_cache.to(torch.float16)
        elif query.dtype == torch.float32:
            # Float32 is supported, will use f32 kernel
            pass
        elif query.dtype == torch.float16:
            # Float16 is natively supported as 'half' in Metal
            pass
        else:
            raise ValueError(f"Unsupported dtype {query.dtype} for Metal attention kernel. "
                           f"Supported: float32, float16, bfloat16")

        # Ensure tensors are on MPS device
        query = query.to('mps') if query.device.type != 'mps' else query
        kv_cache = kv_cache.to('mps') if kv_cache.device.type != 'mps' else kv_cache

        # Get dimensions
        num_tokens, num_heads, head_dim = query.shape
        # Kernel expects 1D output buffer, we'll reshape it later
        output = torch.empty(num_tokens * num_heads * head_dim, device='mps', dtype=query.dtype)

        # Run the attention kernel
        result = self._run_full_attention(
            query, kv_cache, kv_page_indices, kv_page_indptr,
            kv_last_page_lens, qo_indptr, output
        )

        # Convert back to original dtype if needed
        if original_dtype != result.dtype:
            result = result.to(original_dtype)

        # DEBUG MODE: Run PyTorch reference and compare
        if DEBUG_ENABLED:
            from . import debug_utils

            # Run PyTorch reference (use original inputs before dtype conversion)
            # Need to reconstruct original query/kv_cache with original dtype
            query_for_ref = query.to(original_dtype) if query.dtype != original_dtype else query
            kv_cache_for_ref = kv_cache.to(original_dtype) if kv_cache.dtype != original_dtype else kv_cache

            pytorch_output = pytorch_reference.attention_reference(
                query_for_ref, kv_cache_for_ref,
                kv_page_indices, kv_page_indptr, kv_last_page_lens,
                qo_indptr, custom_mask
            )

            # Compare Metal vs PyTorch outputs with dtype-aware tolerances
            # Half-precision requires relaxed tolerances due to different computation order
            if original_dtype == torch.float32:
                atol, rtol = DEBUG_ATOL, DEBUG_RTOL
            elif original_dtype == torch.float16:
                atol, rtol = 5e-4, 5e-3  # Relaxed for float16
            else:  # bfloat16
                atol, rtol = 2e-3, 2e-2  # Relaxed for bfloat16

            matches, diagnostics = debug_utils.compare_tensors(
                result, pytorch_output,
                atol=atol, rtol=rtol,
                operation_name="Attention"
            )

            # Generate and print report
            report = debug_utils.generate_report(
                diagnostics, input_metadata, verbosity=DEBUG_VERBOSITY
            )
            if report:
                print(report)

            # Warning if significant mismatch detected (> 1% of elements)
            # Small mismatch rates (< 1%) are expected for half-precision due to rounding
            if not matches and DEBUG_VERBOSITY >= VERBOSITY_DETAILED:
                mismatch_pct = diagnostics.get('mismatch_percentage', 0)
                if mismatch_pct > 1.0:
                    print(f"WARNING: Attention Metal kernel output differs from PyTorch reference ({mismatch_pct:.2f}% mismatches)")

        return result

    def _run_full_attention(
        self,
        query: torch.Tensor,
        kv_cache: torch.Tensor,
        kv_page_indices: torch.Tensor,
        kv_page_indptr: torch.Tensor,
        kv_last_page_lens: torch.Tensor,
        qo_indptr: torch.Tensor,
        output: torch.Tensor
    ) -> torch.Tensor:
        """Run the actual optimized Metal attention kernel."""

        lib = self.compiled_libraries['attention']

        # Extract dimensions
        num_tokens, num_heads, head_dim = query.shape
        num_pages, _, page_size, num_kv_heads, _ = kv_cache.shape
        scale = 1.0 / (head_dim ** 0.5)

        # Create the Params struct exactly as expected by the Metal kernel
        # This matches what the test uses: 8 parameters
        params_data = [
            num_tokens,                # num_qo (seq_len in test)
            num_heads * head_dim,      # head_dim (total_head_dim in test)
            num_kv_heads * head_dim,   # kv_head_dim (total KV head dim for GQA support)
            head_dim,                  # head_size
            page_size,                 # page_size
            num_heads,                 # num_query_heads
            num_kv_heads,              # num_kv_heads (actual KV heads from cache)
            scale,                     # scale (1/sqrt(head_dim))
        ]

        # Convert to Metal-compatible parameter buffer
        params = torch.tensor(params_data, dtype=torch.float32, device='mps')

        # Prepare tensors in the exact format expected by the kernel
        # The kernel expects 1D flat buffers for K and V caches
        # Input: [num_pages, 2, page_size, num_kv_heads, head_dim]
        # Output: 1D [num_pages * page_size * num_kv_heads * head_dim]
        paged_k_cache = kv_cache[:, 0, :, :, :].contiguous().view(-1)
        paged_v_cache = kv_cache[:, 1, :, :, :].contiguous().view(-1)

        # Reshape query for kernel: [num_tokens, num_heads, head_dim] -> 1D [num_tokens * num_heads * head_dim]
        q_input = query.contiguous().view(-1)

        # Create debug buffer (optional, can be None)
        debug_out = torch.zeros(20, dtype=torch.float32, device='mps')

        # Launch the actual optimized kernel!
        # Select kernel based on dtype: f32 for float32, bf16 for float16/bfloat16
        if query.dtype == torch.float32:
            kernel_name = 'batch_prefill_attention_unified_f32_simdgroup_kernel'
        else:
            kernel_name = 'batch_prefill_attention_unified_bf16_simdgroup_kernel'

        # Configure dispatch parameters
        threads_per_threadgroup = 128
        total_threads = num_tokens * threads_per_threadgroup

        if not hasattr(lib, kernel_name):
            raise RuntimeError(f"Kernel {kernel_name} not found in compiled library")

        # Call the optimized simdgroup kernel with exact parameter layout and dispatch config
        getattr(lib, kernel_name)(
            q_input,                             # q_input [buffer(0)]
            paged_k_cache,                       # paged_k_cache [buffer(1)]
            paged_v_cache,                       # paged_v_cache [buffer(2)]
            qo_indptr.to(torch.int32),          # qo_indptr [buffer(3)]
            kv_page_indptr.to(torch.int32),     # kv_page_indptr [buffer(4)]
            kv_page_indices.to(torch.int32),    # kv_page_indices [buffer(5)]
            kv_last_page_lens.to(torch.int32),  # kv_last_page_lens [buffer(6)]
            output,                              # output [buffer(7)]
            params,                              # params [buffer(8)]
            debug_out,                           # debug_out [buffer(9)]
            threads=(total_threads, 1, 1),
            group_size=(threads_per_threadgroup, 1, 1)
        )

        # Output is already in correct 1D layout written by kernel
        # Kernel writes with stride (num_heads * head_dim) per token
        # Reshape to 2D as per FlashInfer API: [num_tokens, num_heads * head_dim]
        total_head_dim = num_heads * head_dim
        return output.view(num_tokens, total_head_dim)

    def _run_simple_attention(
        self,
        query: torch.Tensor,
        kv_cache: torch.Tensor,
        output: torch.Tensor
    ) -> torch.Tensor:
        """Run simple attention kernel as fallback."""

        lib = self.compiled_libraries['simple']

        num_tokens, num_heads, head_dim = query.shape
        scale = 1.0 / (head_dim ** 0.5)

        # Simple parameters
        params = torch.tensor([
            num_tokens,
            head_dim,
            scale
        ], dtype=torch.float32, device='mps')

        # Flatten tensors for simple kernel
        query_flat = query.view(-1)
        kv_flat = kv_cache.view(-1)[:query_flat.numel()]  # Match size
        value_flat = kv_flat  # Use same as value for simplicity
        output_flat = output.view(-1)

        kernel_name = 'simple_attention_f16' if query.dtype == torch.float16 else 'simple_attention_f32'

        if hasattr(lib, kernel_name):
            getattr(lib, kernel_name)(
                query_flat,
                kv_flat,
                value_flat,
                output_flat,
                params
            )
        else:
            raise RuntimeError(f"Simple kernel {kernel_name} not found")

        return output