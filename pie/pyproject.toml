[build-system]
requires = ["maturin>=1.7,<2.0"]
build-backend = "maturin"

[project]
name = "pie-server"
version = "0.2.0"
description = "Programmable Inference Engine"
readme = "README.md"
requires-python = ">=3.10,<3.13"
license = "Apache-2.0"
authors = [{ name = "In Gim", email = "in.gim@yale.edu" }]
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Rust",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

# Base dependencies (CPU compatible / PyPI available)
dependencies = [
    "einops>=0.8.1",
    "fire>=0.6",
    "httpx>=0.27",
    "huggingface_hub>=1.2.4",
    "numba>=0.63",
    "msgpack>=0.5",
    "msgspec>=0.19.0",
    "opentelemetry-api>=1.20.0",
    "opentelemetry-exporter-otlp-proto-grpc>=1.20.0",
    "opentelemetry-sdk>=1.20.0",
    "pie-client", 
    "platformdirs>=4.3",
    "psutil>=7.1.3",
    "pyzmq>=27.0.0",
    "requests>=2.32",
    "rich>=13.0",
    "safetensors>=0.6.2",
    "toml>=0.10",
    "tqdm>=4.67",
    "typer>=0.12",
    "websockets>=15.0",
    "ztensor>=0.1.4",
]

[project.optional-dependencies]
# --- CUDA Variants ---
cu126 = [
    "torch>=2.7.1",
    "torchao>=0.14.1",  
    "flashinfer-python>=0.6.0",
    "flashinfer-cubin>=0.6.0",
]

cu128 = [
    "torch>=2.7.1",
    "torchao>=0.14.1",  
    "flashinfer-python>=0.6.0",
    "flashinfer-cubin>=0.6.0",
    "flashinfer-jit-cache>=0.6.0",
]

cu130 = [
    "torch>=2.7.1",
    "torchao>=0.14.1",  
    "flashinfer-python>=0.6.0",
    "flashinfer-cubin>=0.6.0",
    "flashinfer-jit-cache>=0.6.0",
]


# --- Metal (macOS) ---
metal = [
    "torch>=2.7.1",
    "pyobjc-core>=10.0,<11.0",
    "pyobjc-framework-Metal>=10.0,<11.0",
    "pyobjc-framework-MetalKit>=10.0,<11.0",
    "pyobjc-framework-Cocoa>=10.0,<11.0",
]

# --- Dev/Test ---
test = ["pytest>=8.0"]

[project.scripts]
pie = "pie_cli.cli:app"

# --- UV Configuration ---

[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true


[[tool.uv.index]]
name = "pytorch-cu130"
url = "https://download.pytorch.org/whl/nightly/cu130"
explicit = true


[[tool.uv.index]]
name = "flashinfer-cu128"
url = "https://flashinfer.ai/whl/cu128"
explicit = true

[[tool.uv.index]]
name = "flashinfer-cu130"
url = "https://flashinfer.ai/whl/cu130"
explicit = true

# 2. Source Pinning
[tool.uv.sources]

# Local workspace member
pie-client = { path = "../client/python", editable = true }

# --- Torch Sources ---
torch = [
    { index = "pytorch-cu126", extra = "cu126" },
    { index = "pytorch-cu130", extra = "cu130" },
]

# --- FlashInfer JIT Cache Sources (only this package uses custom indexes) ---
flashinfer-jit-cache = [
    { index = "flashinfer-cu128", extra = "cu128" },
    { index = "flashinfer-cu130", extra = "cu130" },
]

[tool.maturin] 
python-source = "src"
module-name = "pie._pie"
manifest-path = "../runtime/Cargo.toml"
features = ["pyo3/extension-module"]

[tool.uv]
cache-keys = [{file = "pyproject.toml"}, {file = "../runtime/Cargo.toml"}, {file = "../runtime/**/*.rs"}]

# Declare CUDA/Metal extras as mutually exclusive
conflicts = [
    [
        { extra = "cu126" },
        { extra = "cu128" },
        { extra = "cu130" },
        { extra = "metal" },
    ],
]
